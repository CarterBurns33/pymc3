
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Discourse forum + better docs &#8212; PyMC3 3.5 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<p>We recently released <a class="reference external" href="https://github.com/pymc-devs/pymc3/blob/master/RELEASE-NOTES.md#pymc3-31-june-23-2017">PyMC3
3.1</a>
after the first stable 3.0 release in January 2017. You can update
either via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pymc3</span></code> or via
<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">pymc3</span></code>.</p>
<p>A lot is happening in PyMC3-land. One thing I am particularily proud of
is the developer community we have built. We now have around 10 active
core contributors from the US, Germany, Russia, Japan and Switzerland.
Specifically, since 3.0, Adrian Seyboldt, <a class="reference external" href="http://www.unifr.ch/psycho/staff/lao-junpeng">Junpeng
Lao</a> and Hannes Bathke
have joined the team. Moreover, we have 3 Google Summer of Code
students: <a class="reference external" href="https://ferrine.github.io">Maxime Kochurov</a>, who is
working on Variational Inference; Bill Engels, who is working on
Gaussian Processes, and Bhargav Srinivasa is implementing Riemannian
HMC.</p>
<p>Moreover, PyMC3 is being seeing increased adoption in
<a class="reference external" href="https://scholar.google.de/scholar?hl=en&amp;as_sdt=0,5&amp;sciodt=0,5&amp;cites=6936955228135731011&amp;scipsc=&amp;authuser=1&amp;q=&amp;scisbd=1">academia</a>,
as well as in
<a class="reference external" href="https://github.com/pymc-devs/pymc3/wiki/Testimonials">industry</a>.</p>
<p>Here, I want to highlight some of the new features of PyMC3 3.1.</p>
<div class="section" id="Discourse-forum-+-better-docs">
<h1>Discourse forum + better docs<a class="headerlink" href="#Discourse-forum-+-better-docs" title="Permalink to this headline">¶</a></h1>
<p>To facilitate the community building process and give users a place to
ask questions we have a launched a discourse forum:
<a class="reference external" href="http://discourse.pymc.io">http://discourse.pymc.io</a>. Bug reports should still onto the <a class="reference external" href="https://github.com/pymc-devs/pymc3/issues">Github
issue tracker</a>, but for
all PyMC3 questions or modeling discussions, please use the discourse
forum.</p>
<p>There are also some improvements to the documentation. Mainly, a
<a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/api_quickstart.html">quick-start to the general PyMC3
API</a>,
and a <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/variational_api_quickstart.html">quick-start to the variational
API</a>.</p>
</div>
<div class="section" id="Gaussian-Processes">
<h1>Gaussian Processes<a class="headerlink" href="#Gaussian-Processes" title="Permalink to this headline">¶</a></h1>
<p>PyMC3 now as high-level support for GPs which allow for very flexible
non-linear curve-fitting (among other things). This work was mainly done
by Bill Engels with help from Chris Fonnesbeck. Here, we highlight the
basic API, but for more information see the <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/GP-introduction.html">full
introduction</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="kn">as</span> <span class="nn">cmap</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">cmap</span><span class="o">.</span><span class="n">inferno</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">tt</span>
<span class="kn">import</span> <span class="nn">theano.tensor.nlinalg</span>

<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">20090425</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">))[:,</span><span class="bp">None</span><span class="p">])</span>

<span class="c1"># generate fake data from GP with white noise (with variance sigma2)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">1.36653628</span><span class="p">,</span>  <span class="mf">1.15196999</span><span class="p">,</span>  <span class="mf">0.82142869</span><span class="p">,</span>  <span class="mf">0.85243384</span><span class="p">,</span>  <span class="mf">0.63436304</span><span class="p">,</span>
               <span class="mf">0.14416139</span><span class="p">,</span>  <span class="mf">0.09454237</span><span class="p">,</span>  <span class="mf">0.32878065</span><span class="p">,</span>  <span class="mf">0.51946622</span><span class="p">,</span>  <span class="mf">0.58603513</span><span class="p">,</span>
               <span class="mf">0.46938673</span><span class="p">,</span>  <span class="mf">0.63876778</span><span class="p">,</span>  <span class="mf">0.48415033</span><span class="p">,</span>  <span class="mf">1.28011185</span><span class="p">,</span>  <span class="mf">1.52401102</span><span class="p">,</span>
               <span class="mf">1.38430047</span><span class="p">,</span>  <span class="mf">0.47455605</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.21110139</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.49443319</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.25518805</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># priors on the covariance function hyperparameters and noise</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;l&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">log_s2_f</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;log_s2_f&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">log_s2_n</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;log_s2_n&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">f_cov</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_s2_f</span><span class="p">)</span> <span class="o">*</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">cov</span><span class="o">.</span><span class="n">ExpQuad</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>

    <span class="c1"># Instantiate GP</span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="s1">&#39;y_obs&#39;</span><span class="p">,</span> <span class="n">cov_func</span><span class="o">=</span><span class="n">f_cov</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tt</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_s2_n</span><span class="p">),</span>
                     <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;X&#39;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Draw samples from GP</span>
    <span class="n">gp_samples</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">gp</span><span class="o">.</span><span class="n">sample_gp</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">y_obs</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 27.649:   6%|▌         | 12091/200000 [00:09&lt;02:15, 1386.23it/s]
Convergence archived at 12100
Interrupted at 12,100 [6%]: Average Loss = 9,348
100%|██████████| 1000/1000 [00:20&lt;00:00, 49.74it/s]
100%|██████████| 50/50 [00:12&lt;00:00,  3.93it/s]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="p">[</span><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cm</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">gp_samples</span><span class="p">]</span>
<span class="c1"># overlay the observed data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;f(x)&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Posterior predictive distribution&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_5_0.png" src="../_images/notebooks_new_in_pymc3_3.1_5_0.png" />
</div>
</div>
</div>
<div class="section" id="Improvements-to-NUTS">
<h1>Improvements to NUTS<a class="headerlink" href="#Improvements-to-NUTS" title="Permalink to this headline">¶</a></h1>
<p>NUTS is now identical to Stan’s implementation and also much much
faster. In addition, Adrian Seyboldt added <a class="reference external" href="http://pymc-devs.github.io/pymc3/api/inference.html#module-pymc3.step_methods.hmc.nuts">higher-order
integrators</a>,
which promise to be more efficient in higher dimensions, and <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/sampler-stats.html">sampler
statistics</a>
that help identify problems with NUTS sampling.</p>
<p>In addition, we changed the default kwargs of <code class="docutils literal notranslate"><span class="pre">pm.sample()</span></code>. By
default, the sampler is run for 500 iterations with tuning enabled (you
can change this with the <code class="docutils literal notranslate"><span class="pre">tune</span></code> kwarg), these samples are then
discarded from the returned trace. Moreover, if no arguments are
specified, <code class="docutils literal notranslate"><span class="pre">sample()</span></code> will draw 500 samples in addition to the tuning
samples. So for almost all models, just calling <code class="docutils literal notranslate"><span class="pre">pm.sample()</span></code> should
be sufficient.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu1&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># do not remove tuned samples for the plot below</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 7.279:  14%|█▍        | 28648/200000 [00:08&lt;00:53, 3176.47it/s]
Convergence archived at 28900
Interrupted at 28,900 [14%]: Average Loss = 8.9536
100%|██████████| 1000/1000 [00:03&lt;00:00, 263.60it/s]
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">trace</span></code> now has a bunch of extra parameters pertaining to statistics
of the sampler:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;step_size_bar&#39;</span><span class="p">]);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;step size&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_9_0.png" src="../_images/notebooks_new_in_pymc3_3.1_9_0.png" />
</div>
</div>
</div>
<div class="section" id="Variational-Inference">
<h1>Variational Inference<a class="headerlink" href="#Variational-Inference" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://ferrine.github.io">Maxim “Ferrine” Kochurov</a> has done
outstanding contributions to improve support for Variational Inference.
Essentially, Ferrine has implemented <a class="reference external" href="https://arxiv.org/abs/1610.09033">Operator Variational Inference
(OPVI)</a> which is a framework to
express many existing VI approaches in a modular fashion. He has also
made it much easier to supply mini-batches. See
<a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/variational_api_quickstart.html">here</a>
for a full overview of the capabilities.</p>
<p>Specifically, PyMC3 supports the following VI methods: * Auto-diff
Variational Inference (ADVI) mean-field * ADVI full rank * Stein
Variational Gradient Descent (SVGD) * Armortized SVGD</p>
<p>In addition, Ferrine is making great progress on adding Flows which
allows learning very flexible transformations of the VI approximation to
learn more complex (i.e. non-normal) posterior distributions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">x_mini</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Minibatch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sd&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">x_mini</span><span class="p">)</span>
    <span class="n">vi_est</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span> <span class="c1"># Run ADVI</span>
    <span class="n">vi_trace</span> <span class="o">=</span> <span class="n">vi_est</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># sample from VI posterior</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Average Loss = 149.38: 100%|██████████| 10000/10000 [00:01&lt;00:00, 9014.13it/s]
Finished [100%]: Average Loss = 149.33
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vi_est</span><span class="o">.</span><span class="n">hist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ELBO&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_12_0.png" src="../_images/notebooks_new_in_pymc3_3.1_12_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">vi_trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_13_0.png" src="../_images/notebooks_new_in_pymc3_3.1_13_0.png" />
</div>
</div>
<p>As you can see, we have also added a new high-level API in the spirit of
<code class="docutils literal notranslate"><span class="pre">sample</span></code>: <code class="docutils literal notranslate"><span class="pre">pymc3.fit()</span></code> with many configuration options:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">help</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Help on function fit in module pymc3.variational.inference:

fit(n=10000, local_rv=None, method=&#39;advi&#39;, model=None, random_seed=None, start=None, inf_kwargs=None, **kwargs)
    Handy shortcut for using inference methods in functional way

    Parameters
    ----------
    n : `int`
        number of iterations
    local_rv : dict[var-&gt;tuple]
        mapping {model_variable -&gt; local_variable (:math:`\mu`, :math:`\rho`)}
        Local Vars are used for Autoencoding Variational Bayes
        See (AEVB; Kingma and Welling, 2014) for details
    method : str or :class:`Inference`
        string name is case insensitive in {&#39;advi&#39;, &#39;fullrank_advi&#39;, &#39;advi-&gt;fullrank_advi&#39;, &#39;svgd&#39;, &#39;asvgd&#39;}
    model : :class:`pymc3.Model`
        PyMC3 model for inference
    random_seed : None or int
        leave None to use package global RandomStream or other
        valid value to create instance specific one
    inf_kwargs : dict
        additional kwargs passed to :class:`Inference`
    start : `Point`
        starting point for inference

    Other Parameters
    ----------------
    frac : `float`
        if method is &#39;advi-&gt;fullrank_advi&#39; represents advi fraction when training
    kwargs : kwargs
        additional kwargs for :func:`Inference.fit`

    Returns
    -------
    :class:`Approximation`

</pre></div></div>
</div>
<p>SVGD for example is an algorithm that updates multiple particles and is
thus well suited for multi-modal posteriors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">NormalMixture</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span>
                     <span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">]),</span>
                     <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]),</span>
                     <span class="n">sd</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">vi_est</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;SVGD&#39;</span><span class="p">)</span>
    <span class="n">vi_est</span> <span class="o">=</span> <span class="n">vi_est</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
100%|██████████| 10000/10000 [00:24&lt;00:00, 407.10it/s]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">vi_est</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[50]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x12f335208&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_18_1.png" src="../_images/notebooks_new_in_pymc3_3.1_18_1.png" />
</div>
</div>
</div>
<div class="section" id="Cholesky-factorization">
<h1>Cholesky factorization<a class="headerlink" href="#Cholesky-factorization" title="Permalink to this headline">¶</a></h1>
<p>There is a nice trick to covariance estimation using the <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky
decomposition</a>
for increased efficiency and numerical stability. The <code class="docutils literal notranslate"><span class="pre">MvNormal</span></code>
distribution now accepts a Cholesky-factored covariance matrix. In
addition, the LKJ prior has been changed to provide the Cholesky
covariance matrix. Thus, if you are estimating covariances, definitely
use this much improved parameterization.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Note that we access the distribution for the standard</span>
    <span class="c1"># deviations, and do not create a new random variable.</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
    <span class="n">packed_chol</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s1">&#39;chol_cov&#39;</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_dim</span><span class="p">,</span>
                                    <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">chol</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">expand_packed_triangular</span><span class="p">(</span><span class="n">n_dim</span><span class="p">,</span> <span class="n">packed_chol</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Define a new MvNormal with the given covariance</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s1">&#39;vals&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_dim</span><span class="p">),</span>
                       <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_dim</span><span class="p">,</span>
                       <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 716.37:   8%|▊         | 15309/200000 [00:05&lt;01:06, 2768.80it/s]
Convergence archived at 15400
Interrupted at 15,400 [7%]: Average Loss = 2,171.4
100%|██████████| 1000/1000 [00:06&lt;00:00, 160.64it/s]
</pre></div></div>
</div>
</div>
<div class="section" id="Live-trace-to-see-sampling-in-real-time">
<h1>Live-trace to see sampling in real-time<a class="headerlink" href="#Live-trace-to-see-sampling-in-real-time" title="Permalink to this headline">¶</a></h1>
<p>This one is really cool, you can watch the trace evolve while its
sampling using <code class="docutils literal notranslate"><span class="pre">pm.sample(live_plot=True)</span></code>. Contributed by David
Brochart. See
<a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/live_sample_plots.html">here</a>
for full docs.</p>
</div>
<div class="section" id="Better-display-of-random-variables">
<h1>Better display of random variables<a class="headerlink" href="#Better-display-of-random-variables" title="Permalink to this headline">¶</a></h1>
<p>We now make use of the fancy display features of the Jupyter Notebook to
provide a nicer view of RVs:</p>
</div>
<div class="section" id="Sequential-Monte-Carlo-(experimental)">
<h1>Sequential Monte Carlo (experimental)<a class="headerlink" href="#Sequential-Monte-Carlo-(experimental)" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://github.com/hvasbath">Hannes Bathke</a> has implemented a new
SMC, or particle sampler, which is very useful if no gradients are
available for your model or the posterior is multi-modal.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [106]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">!</span>rm -rf /tmp/stage*
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [107]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">NormalMixture</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span>
                     <span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]),</span>
                     <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">]),</span>
                     <span class="n">sd</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">smc</span><span class="o">.</span><span class="n">SMC</span><span class="p">(</span><span class="n">n_chains</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">n_steps</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">smc</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">homepath</span><span class="o">=</span><span class="s1">&#39;/tmp&#39;</span><span class="p">)</span>
    <span class="c1">#trace = pm.sample()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
../../../pymc3/step_methods/smc.py:120: UserWarning: Warning: SMC is an experimental step method, and not yet recommended for use in PyMC3!
  warnings.warn(EXPERIMENTAL_WARNING)
../../../pymc3/step_methods/smc.py:474: UserWarning: Warning: SMC is an experimental step method, and not yet recommended for use in PyMC3!
  warnings.warn(EXPERIMENTAL_WARNING)
Init new trace!
Sample initial stage: ...
Beta: 0.000000 Stage: 0
Initialising chain traces ...
Sampling ...
Beta &gt; 1.: 1.999999
Sample final stage
Initialising chain traces ...
Sampling ...
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [108]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_26_0.png" src="../_images/notebooks_new_in_pymc3_3.1_26_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [99]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">last_sample</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">last_sample</span><span class="p">,</span> <span class="n">combined</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
<span class="c1">#pm.traceplot(trace);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_new_in_pymc3_3.1_27_0.png" src="../_images/notebooks_new_in_pymc3_3.1_27_0.png" />
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="err">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;μ&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="err">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;σ&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="err">γ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;γ&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="err">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="err">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="err">γ</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="math notranslate nohighlight">
$γ \sim \text{Normal}(\mathit{mu}=μ, \mathit{sd}=f(σ))$</div></div>
</div>
<p>You can create greek letters in the Jupyter Notebook by typing the
<span class="math notranslate nohighlight">\(\LaTeX\)</span> command and hitting tab: <a href="#id1"><span class="problematic" id="id2">:raw-latex:`\mu[tab]`</span></a>.</p>
</div>
<div class="section" id="GPU-support-(experimental)">
<h1>GPU support (experimental)<a class="headerlink" href="#GPU-support-(experimental)" title="Permalink to this headline">¶</a></h1>
<p>While still experimental, we have made a lot of progress towards fully
supporting <code class="docutils literal notranslate"><span class="pre">float32</span></code> throughout. If you set <code class="docutils literal notranslate"><span class="pre">floatX</span> <span class="pre">=</span> <span class="pre">float32</span></code> in
your <code class="docutils literal notranslate"><span class="pre">.theanorc</span></code>, cast all your input data to <code class="docutils literal notranslate"><span class="pre">float32</span></code> (e.g. by
using <code class="docutils literal notranslate"><span class="pre">pm.floatX()</span></code> for automatic casting), you should get much faster
inference. If you set the backend to use the GPU, you should get a nice
speed-up on the right types of models. Please report any successes or
failures in this regard.</p>
</div>
<div class="section" id="Other-useful-packages">
<h1>Other useful packages<a class="headerlink" href="#Other-useful-packages" title="Permalink to this headline">¶</a></h1>
<p>These are not part of PyMC3 3.1 but build on it and should be of
interest to many users.</p>
<div class="section" id="Bayesian-Deep-Learning-with-Gelato">
<h2>Bayesian Deep Learning with Gelato<a class="headerlink" href="#Bayesian-Deep-Learning-with-Gelato" title="Permalink to this headline">¶</a></h2>
<p>Gelato bridges PyMC3 and Lasagne, a library to easily build Neural
Networks similar to Keras. Building Bayesian convolution neural networks
and estimating them using VI has never been simpler. See
<a class="reference external" href="https://github.com/ferrine/gelato/blob/master/examples/mnist.ipynb">here</a>
for an example on MNIST.</p>
</div>
<div class="section" id="Building-hierarchical-GLMs-with-Bambi">
<h2>Building hierarchical GLMs with Bambi<a class="headerlink" href="#Building-hierarchical-GLMs-with-Bambi" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/bambinos/bambi">Bambi</a> is a new package on top of
PyMC3 (they also recently added a Stan backend) which allows creation of
complex, hierarchical GLMs with very intuitive syntax, e.g.:</p>
<p><code class="docutils literal notranslate"><span class="pre">model.fit('rt</span> <span class="pre">~</span> <span class="pre">condition',</span> <span class="pre">random=['condition|subject',</span> <span class="pre">'1|stimulus'],</span> <span class="pre">samples=5000,</span> <span class="pre">chains=2)</span></code>.</p>
</div>
</div>
<div class="section" id="Looking-towards-PyMC3-3.2">
<h1>Looking towards PyMC3 3.2<a class="headerlink" href="#Looking-towards-PyMC3-3.2" title="Permalink to this headline">¶</a></h1>
<p>PyMC3 3.2 is already underway. Some of the features we are working on
include: * <a class="reference external" href="https://github.com/pymc-devs/pymc3/pull/2345">faster sampling on the
GPU</a>, * <a class="reference external" href="https://github.com/pymc-devs/pymc3/pull/1977">Stochastic
Gradient Fisher Scoring for scalable min-batch
MCMC</a>, * <a class="reference external" href="https://github.com/pymc-devs/pymc3/pull/2362">Normalizing
Flows</a> for flexible
variational inference on non-normal posteriors, * <a class="reference external" href="https://github.com/pymc-devs/pymc3/pull/2322">scalable
GPs</a>, * <cite>support for
the ``emcee`</cite> sampler &lt;External%20emcee%20sampler%20support&gt;`__, and *
<a class="reference external" href="https://github.com/pymc-devs/pymc3/pull/2240">Riemannian HMC</a> for
efficient sampling in high-dimensional complex spaces.</p>
</div>
<div class="section" id="On-my-own-behalf:-Patreon">
<h1>On my own behalf: Patreon<a class="headerlink" href="#On-my-own-behalf:-Patreon" title="Permalink to this headline">¶</a></h1>
<p>I have recently created an account on Patreon where you can <a class="reference external" href="https://www.patreon.com/twiecki">support me
financially</a> for writing blog posts.
These allow me to devote more time to writing posts so if you find this
blog useful, please consider supporting me.</p>
<p>Thanks specifically to <a class="reference external" href="https://twitter.com/_jonathanng_">Jonathan
Ng</a> for pledging.</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/notebooks/new_in_pymc3_3.1.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>