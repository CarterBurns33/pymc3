
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>The intuitive specification &#8212; PyMC3 3.5 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<ol class="loweralpha simple" start="3">
<li>2017 by Thomas Wiecki</li>
</ol>
<p>Hierarchical models are underappreciated. Hierarchies exist in many data
sets and modeling them appropriately adds a boat load of statistical
power (the common metric of statistical power). I provided an
introduction to hierarchical models in a <a class="reference external" href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">previous blog post: Best Of
Both Worlds: Hierarchical Linear Regression in
PyMC3”</a>,
written with Danne Elbers. See also my <a class="reference external" href="http://blog.fastforwardlabs.com/2017/01/11/thomas-wiecki-on-probabilistic-programming-with.html">interview with
FastForwardLabs</a>
where I touch on these points.</p>
<p>Here I want to focus on a common but subtle problem when trying to
estimate these models and how to solve it with a simple trick. Although
I was somewhat aware of this trick for quite some time it just recently
clicked for me. We will use the same hierarchical linear regression
model on the Radon data set from the previous blog post, so if you are
not familiar, I recommend to start there.</p>
<p>I will then use the intuitions we’ve built up to highlight a subtle
point about expectations vs modes (i.e. the MAP). Several talks by
<a class="reference external" href="https://twitter.com/betanalpha">Michael Betancourt</a> have really
expanded my thinking here.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;whitegrid&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/radon.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;log_radon&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;log_radon&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>
<span class="n">county_names</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">county</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="n">county_idx</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">county_code</span><span class="o">.</span><span class="n">values</span>

<span class="n">n_counties</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">county</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="The-intuitive-specification">
<h1>The intuitive specification<a class="headerlink" href="#The-intuitive-specification" title="Permalink to this headline">¶</a></h1>
<p>Usually, hierachical models are specified in a <em>centered</em> way. In a
regression model, individual slopes would be centered around a group
mean with a certain group variance, which controls the shrinkage:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">hierarchical_model_centered</span><span class="p">:</span>
    <span class="c1"># Hyperpriors for group nodes</span>
    <span class="n">mu_a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;mu_a&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">100</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma_a&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">mu_b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;mu_b&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">100</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Intercept for each county, distributed around group mean mu_a</span>
    <span class="c1"># Above we just set mu and sd to a fixed value while here we</span>
    <span class="c1"># plug in a common group distribution for all a and b (which are</span>
    <span class="c1"># vectors of length n_counties).</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_a</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma_a</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_counties</span><span class="p">)</span>

    <span class="c1"># Intercept for each county, distributed around group mean mu_a</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_b</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma_b</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_counties</span><span class="p">)</span>

    <span class="c1"># Model error</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Linear regression</span>
    <span class="n">radon_est</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">county_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">county_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">floor</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Data likelihood</span>
    <span class="n">radon_like</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;radon_like&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">radon_est</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">log_radon</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Inference button (TM)!</span>
<span class="k">with</span> <span class="n">hierarchical_model_centered</span><span class="p">:</span>
    <span class="n">hierarchical_centered_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)[</span><span class="mi">1000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 2 jobs)
NUTS: [eps_log__, b, a, sigma_b_log__, mu_b, sigma_a_log__, mu_a]
100%|██████████| 6000/6000 [00:06&lt;00:00, 989.54it/s]
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
The estimated number of effective samples is smaller than 200 for some parameters.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_5_0.png" src="../_images/notebooks_GLM-hierarchical_centered_5_0.png" />
</div>
</div>
<p>I have seen plenty of traces with terrible convergences but this one
might look fine to the unassuming eye. Perhaps <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> has some
problems, so let’s look at the Rhat:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Rhat(sigma_b) = {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">gelman_rubin</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">)[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Rhat(sigma_b) = 2451246.683155101
</pre></div></div>
</div>
<p>Not too bad – well below 1.01. I used to think this wasn’t a big deal
but Michael Betancourt in his <a class="reference external" href="https://www.youtube.com/watch?v=DJ0c7Bm5Djk&amp;feature=youtu.be&amp;t=4h40m9s">StanCon 2017
talk</a>
makes a strong point that it is actually very problematic. To understand
what’s going on, let’s take a closer look at the slopes <code class="docutils literal notranslate"><span class="pre">b</span></code> and their
group variance (i.e. how far they are allowed to move from the mean)
<code class="docutils literal notranslate"><span class="pre">sigma_b</span></code>. I’m just plotting a single chain now.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_9_0.png" src="../_images/notebooks_GLM-hierarchical_centered_9_0.png" />
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> seems to drift into this area of very small values and get
stuck there for a while. This is a common pattern and the sampler is
trying to tell you that there is a region in space that it can’t quite
explore efficiently. While stuck down there, the slopes <code class="docutils literal notranslate"><span class="pre">b_i</span></code> become
all squished together. We’ve entered <strong>The Funnel of Hell</strong> (it’s just
called the funnel, I added the last part for dramatic effect).</p>
</div>
<div class="section" id="The-Funnel-of-Hell-(and-how-to-escape-it)">
<h1>The Funnel of Hell (and how to escape it)<a class="headerlink" href="#The-Funnel-of-Hell-(and-how-to-escape-it)" title="Permalink to this headline">¶</a></h1>
<p>Let’s look at the joint posterior of a single slope <code class="docutils literal notranslate"><span class="pre">b</span></code> (I randomly
chose the 75th one) and the slope group variance <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">][:,</span> <span class="mi">75</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope b_75&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope group variance sigma_b&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_12_0.png" src="../_images/notebooks_GLM-hierarchical_centered_12_0.png" />
</div>
</div>
<p>This makes sense, as the slope group variance goes to zero (or, said
differently, we apply maximum shrinkage), individual slopes are not
allowed to deviate from the slope group mean, so they all collapose to
the group mean.</p>
<p>While this property of the posterior in itself is not problematic, it
makes the job extremely difficult for our sampler. Imagine a
<a class="reference external" href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/">Metropolis-Hastings</a>
exploring this space with a medium step-size (we’re using NUTS here but
the intuition works the same): in the wider top region we can
comfortably make larger jumps to explore the space efficiently. However,
once we move to the narrow bottom region we can change <code class="docutils literal notranslate"><span class="pre">b_75</span></code> and
<code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> only by tiny amounts. This causes the sampler to become
trapped in that region of space. Most of the proposals will be rejected
because our step-size is too large for this narrow part of the space and
exploration will be very inefficient.</p>
<p>You might wonder if we could somehow choose the step-size based on the
denseness (or curvature) of the space. Indeed that’s possible and it’s
called <a class="reference external" href="https://arxiv.org/abs/0907.1100">Riemannian HMC</a>. It works
very well but is quite costly to run. Here, we will explore a different,
simpler method.</p>
<p>Finally, note that this problem does not exist for the intercept
parameters <code class="docutils literal notranslate"><span class="pre">a</span></code>. Because we can determine individual intercepts <code class="docutils literal notranslate"><span class="pre">a_i</span></code>
with enough confidence, <code class="docutils literal notranslate"><span class="pre">sigma_a</span></code> is not small enough to be
problematic. Thus, the funnel of hell can be a problem in hierarchical
models, but it does not have to be. (Thanks to John Hall for pointing
this out).</p>
</div>
<div class="section" id="Reparameterization">
<h1>Reparameterization<a class="headerlink" href="#Reparameterization" title="Permalink to this headline">¶</a></h1>
<p>If we can’t easily make the sampler step-size adjust to the region of
space, maybe we can adjust the region of space to make it simpler for
the sampler? This is indeed possible and quite simple with a small
reparameterization trick, we will call this the <em>non-centered</em> version.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">hierarchical_model_non_centered</span><span class="p">:</span>
    <span class="c1"># Hyperpriors for group nodes</span>
    <span class="n">mu_a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;mu_a&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">100</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma_a&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">mu_b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;mu_b&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">100</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Before:</span>
    <span class="c1"># a = pm.Normal(&#39;a&#39;, mu=mu_a, sd=sigma_a, shape=n_counties)</span>
    <span class="c1"># Transformed:</span>
    <span class="n">a_offset</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;a_offset&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_counties</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">mu_a</span> <span class="o">+</span> <span class="n">a_offset</span> <span class="o">*</span> <span class="n">sigma_a</span><span class="p">)</span>

    <span class="c1"># Before:</span>
    <span class="c1"># b = pm.Normal(&#39;b&#39;, mu=mu_b, sd=sigma_b, shape=n_counties)</span>
    <span class="c1"># Now:</span>
    <span class="n">b_offset</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b_offset&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_counties</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">mu_b</span> <span class="o">+</span> <span class="n">b_offset</span> <span class="o">*</span> <span class="n">sigma_b</span><span class="p">)</span>

    <span class="c1"># Model error</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="n">radon_est</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">county_idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">county_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">floor</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Data likelihood</span>
    <span class="n">radon_like</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;radon_like&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">radon_est</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">log_radon</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Pay attention to the definitions of <code class="docutils literal notranslate"><span class="pre">a_offset</span></code>, <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b_offset</span></code>,
and <code class="docutils literal notranslate"><span class="pre">b</span></code> and compare them to before (commented out). What’s going on
here? It’s pretty neat actually. Instead of saying that our individual
slopes <code class="docutils literal notranslate"><span class="pre">b</span></code> are normally distributed around a group mean (i.e. modeling
their absolute values directly), we can say that they are offset from a
group mean by a certain value (<code class="docutils literal notranslate"><span class="pre">b_offset</span></code>; i.e. modeling their values
relative to that mean). Now we still have to consider how far from that
mean we actually allow things to deviate (i.e. how much shrinkage we
apply). This is where <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> makes a comeback. We can simply
multiply the offset by this scaling factor to get the same effect as
before, just under a different parameterization. For a more formal
introduction, see e.g. <a class="reference external" href="https://arxiv.org/pdf/1312.0906.pdf">Betancourt &amp; Girolami
(2013)</a>.</p>
<p>Critically, <code class="docutils literal notranslate"><span class="pre">b_offset</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> are now mostly independent.
This will become more clear soon. Let’s first look at if this transform
helped our sampling:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Inference button (TM)!</span>
<span class="k">with</span> <span class="n">hierarchical_model_non_centered</span><span class="p">:</span>
    <span class="n">hierarchical_non_centered_trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">njobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)[</span><span class="mi">1000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [eps_log__, b_offset, a_offset, sigma_b_log__, mu_b, sigma_a_log__, mu_a]
100%|██████████| 6000/6000 [00:09&lt;00:00, 639.94it/s]
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The acceptance probability does not match the target. It is 1.0, but should be close to 0.8. Try to increase the number of tuning steps.
The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
The estimated number of effective samples is smaller than 200 for some parameters.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">,</span> <span class="n">varnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_17_0.png" src="../_images/notebooks_GLM-hierarchical_centered_17_0.png" />
</div>
</div>
<p>That looks much better as also confirmed by the joint plot:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">][:,</span> <span class="mi">75</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope b_75&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope group variance sigma_b&#39;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Centered&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;b_75&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">][:,</span> <span class="mi">75</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope b_75&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope group variance sigma_b&#39;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Non-centered&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;b_75&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_19_0.png" src="../_images/notebooks_GLM-hierarchical_centered_19_0.png" />
</div>
</div>
<p>To really drive this home, let’s also compare the <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> marginal
posteriors of the two models:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">hierarchical_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Centered&#39;</span><span class="p">,</span> <span class="s1">&#39;Non-cenetered&#39;</span><span class="p">,</span> <span class="s1">&#39;Centered posterior mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Non-centered posterior mean&#39;</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability Density&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_21_0.png" src="../_images/notebooks_GLM-hierarchical_centered_21_0.png" />
</div>
</div>
<p>That’s crazy – there’s a large region of very small <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> values
that the sampler could not even explore before. In other words, our
previous inferences (“Centered”) were severely biased towards higher
values of <code class="docutils literal notranslate"><span class="pre">sigma_b</span></code>. Indeed, if you look at the <a class="reference external" href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">previous blog
post</a> the
sampler never even got stuck in that low region causing me to believe
everything was fine. These issues are hard to detect and very subtle,
but they are meaningful as demonstrated by the sizable difference in
posterior mean.</p>
<p>But what does this concretely mean for our analysis? Over-estimating
<code class="docutils literal notranslate"><span class="pre">sigma_b</span></code> means that we have a biased (=false) belief that we can tell
individual slopes apart better than we actually can. There is less
information in the individual slopes than what we estimated.</p>
<div class="section" id="Why-does-the-reparameterized-model-work-better?">
<h2>Why does the reparameterized model work better?<a class="headerlink" href="#Why-does-the-reparameterized-model-work-better?" title="Permalink to this headline">¶</a></h2>
<p>To more clearly understand why this model works better, let’s look at
the joint distribution of <code class="docutils literal notranslate"><span class="pre">b_offset</span></code>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;b_offset&#39;</span><span class="p">][:,</span> <span class="mi">75</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope b_offset_75&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;slope group variance sigma_b&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[13]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;seaborn.axisgrid.JointGrid at 0x1c1a211588&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-hierarchical_centered_24_1.png" src="../_images/notebooks_GLM-hierarchical_centered_24_1.png" />
</div>
</div>
<p>This is the space the sampler sees; you can see how the funnel is
flattened out. We can freely change the (relative) slope offset
parameters even if the slope group variance is tiny as it just acts as a
scaling parameter.</p>
<p>Note that the funnel is still there – it’s a perfectly valid property
of the model – but the sampler has a much easier time exploring it in
this different parameterization.</p>
</div>
</div>
<div class="section" id="Why-hierarchical-models-are-Bayesian">
<h1>Why hierarchical models are Bayesian<a class="headerlink" href="#Why-hierarchical-models-are-Bayesian" title="Permalink to this headline">¶</a></h1>
<p>Finally, I want to take the opportunity to make another point that is
not directly related to hierarchical models but can be demonstrated
quite well here.</p>
<p>Usually when talking about the perils of Bayesian statistics we talk
about priors, uncertainty, and flexibility when coding models using
Probabilistic Programming. However, an even more important property is
rarely mentioned because it is much harder to communicate. Ross Taylor
touched on this point in his tweet:</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><p>It’s interesting that many summarize Bayes as being about priors; but
real power is its focus on integrals/expectations over maxima/modes</p>
</p><p>— Ross Taylor (&#64;rosstaylor90) February 2, 2017</p>
</blockquote><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><p>Michael Betancourt makes a similar point when he says <a class="reference external" href="https://www.youtube.com/watch?v=pHsuIaPbNbY&amp;t=8s">“Expectations are
the only thing that make
sense.”</a></p>
<p>But what’s wrong with maxima/modes? Aren’t those really close to the
posterior mean (i.e. the expectation)? Unfortunately, that’s only the
case for the simple models we teach to build up intuitions. In complex
models, like the hierarchical one, the MAP can be far away and not be
interesting or meaningful at all.</p>
<p>Let’s compare the posterior mode (i.e. the MAP) to the posterior mean of
our hierachical linear regression model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">hierarchical_model_centered</span><span class="p">:</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">find_MAP</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
logp = -722.97, ||grad|| = 3.2441e+08: 100%|██████████| 258/258 [00:00&lt;00:00, 521.63it/s]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">mode</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[15]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([  7.37621635e-02,  -4.84074831e-01,   3.84360960e+07,
        -2.04290882e-01,   3.84360960e+07,   9.07566398e-02,
         6.41667023e-02,   7.77171478e-02,   1.45614266e-01,
        -1.72194406e-01,   9.07566398e-02,   9.07566398e-02,
         9.07566398e-02,  -5.87148905e-01,   1.91996973e-02,
         9.07566398e-02,  -1.72253653e-01,   3.84360960e+07,
        -3.80501412e-02,   9.07566398e-02,   3.84360960e+07,
         3.84360960e+07,   3.84360960e+07,  -9.13841575e-02,
         4.88621332e-02,  -3.84360960e+07,   3.84360960e+07,
         3.84360960e+07,   9.07566398e-02,   9.07566398e-02,
         9.07566398e-02,   9.07566398e-02,   9.07566398e-02,
         3.84360960e+07,   3.84360960e+07,   3.84360960e+07,
         3.84360960e+07,   1.19797513e-01,   3.84360960e+07,
         7.56382272e-02,   3.84360960e+07,   9.07566398e-02,
         3.84360960e+07,   7.68721920e+07,  -5.98397732e-01,
         9.07566398e-02,   3.84360960e+07,   3.84360960e+07,
         3.84360960e+07,   9.07566398e-02,   9.07566398e-02,
         9.07566398e-02,   3.84360960e+07,  -7.40364015e-01,
        -4.34128679e-02,   3.84360960e+07,   3.84360960e+07,
         1.24754541e-01,  -4.73423935e-02,   9.07566398e-02,
        -1.75111845e-01,   3.27257991e-01,   1.93599626e-01,
        -6.70020580e-02,   9.07566398e-02,  -7.68721920e+07,
        -7.59681761e-02,   9.07566398e-02,   9.07566398e-02,
         6.87542391e+00,  -3.84360960e+07,   9.07566398e-02,
         9.07566398e-02,   9.07566398e-02,   1.96703061e-01,
         3.84360960e+07,   3.84360960e+07,  -9.38449204e-02,
         3.84360960e+07,  -7.68721920e+07,   3.24759752e-01,
         9.07566398e-02,   3.84360960e+07,  -7.36938268e-02,
         9.07566398e-02], dtype=float32)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">mode</span><span class="p">[</span><span class="s1">&#39;sigma_b_log_&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyError</span>                                  Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-16-b0895c2a7934&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>np<span class="ansi-blue-fg">.</span>exp<span class="ansi-blue-fg">(</span>mode<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">&#39;sigma_b_log_&#39;</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">KeyError</span>: &#39;sigma_b_log_&#39;
</pre></div></div>
</div>
<p>As you can see, the slopes are all identical and the group slope
variance is effectively zero. The reason is again related to the funnel.
The MAP only cares about the probability <strong>density</strong> which is highest at
the bottom of the funnel.</p>
<p>But if you could only choose one point in parameter space to summarize
the posterior above, would this be the one you’d pick? Probably not.</p>
<p>Let’s instead look at the <strong>Expected Value</strong> (i.e. posterior mean) which
is computed by integrating probability <strong>density</strong> and <strong>volume</strong> to
provide probabilty <strong>mass</strong> – the thing we really care about. Under the
hood, that’s the integration performed by the MCMC sampler.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">hierarchical_non_centered_trace</span><span class="p">[</span><span class="s1">&#39;sigma_b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Quite a difference. This also explains why it can be a bad idea to use
the MAP to initialize your sampler: in certain models the MAP is not at
all close to the region you want to explore (i.e. the “typical set”).</p>
<p>This strong divergence of the MAP and the Posterior Mean does not only
happen in hierarchical models but also in high dimensional ones, where
our intuitions from low-dimensional spaces gets twisted in serious ways.
<a class="reference external" href="https://www.youtube.com/watch?v=pHsuIaPbNbY&amp;t=8s">This talk by Michael
Betancourt</a> makes
the point quite nicely.</p>
<p>So why do people – especially in Machine Learning – still use the
MAP/MLE? As we all learned in high school first hand, integration is
much harder than differentation. This is really the only reason.</p>
<p>Final disclaimer: This might provide the impression that this is a
property of being in a Bayesian framework, which is not true.
Technically, we can talk about Expectations vs Modes irrespective of
that. Bayesian statistics just happens to provide a very intuitive and
flexible framework for expressing and estimating these models.</p>
<p>See
<a class="reference external" href="https://rawgithub.com/twiecki/WhileMyMCMCGentlySamples/master/content/downloads/notebooks/GLM_hierarchical_non_centered.ipynb">here</a>
for the underlying notebook of this blog post.</p>
</div>
<div class="section" id="Acknowledgements">
<h1>Acknowledgements<a class="headerlink" href="#Acknowledgements" title="Permalink to this headline">¶</a></h1>
<p>Thanks to <a class="reference external" href="https://twitter.com/jonsedar">Jon Sedar</a> for helpful
comments on an earlier draft.</p>
</div>
<div class="section" id="Dependencies">
<h1>Dependencies<a class="headerlink" href="#Dependencies" title="Permalink to this headline">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -dmvgp numpy,scipy,pandas,matplotlib,seaborn,patsy,pymc3,theano,joblib
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/notebooks/GLM-hierarchical_centered.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>